{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from pathlib import Path\n",
    "from jinja import Jinja, Documents\n",
    "from instruct import instruct\n",
    "from lettreSetting import appform\n",
    "from IPython.display import display,Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'file': 'argument2_valid.j2', 'params': ['argument']},\n",
       " {'file': 'argument_valid.j2', 'params': ['argument']},\n",
       " {'file': 'output_json.j2', 'params': []},\n",
       " {'file': 'output_markdown.j2', 'params': ['response']},\n",
       " {'file': 'output_md2html.j2', 'params': ['markdown']},\n",
       " {'file': 'quarto_simple.j2', 'params': ['title', 'author', 'result']}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_list = [\"groq-llama3\", \"claude-haiku\", \"qwen\", \"claude-opus\", \"gpt-4o\",\"command-r\"]\n",
    "model=\"groq-llama3\"\n",
    "model=\"llama3-70\"\n",
    "d = Documents(\"./documents\")\n",
    "t = Jinja(\"./templates\")\n",
    "display(t.list())\n",
    "msg = d.source[\"sysmsg\"][0][\"text\"]\n",
    "text=d.source[\"arguments\"][3][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'file': 'argument2_valid.j2',\n",
       "  'content': 'Use the method of truth-tables to determine, whether a given as sentences between tags <argument> argument </argument>  is\\nvalid or not. \\n\\n<argument>\\n{{argument}}\\n</argument>\\n \\nExecute steps of reasoning:\\n\\nAn argument consists of the conjunction of all assumptions which imply the conclusion. Execute all steps:\\n\\n    - Step 1: tabulate a table T1 of the sentences. Each row is one sentence as assumption except the last row which is the conclusion. \\n    - Step 2: create a table T2 from T1 with a second column \"complex\" which each sentence \\n      decomposes in its logical functions of propositions an their operators \"and\", \"or\", \"if-then\", \"not\". \\n      The same proposition, even as part of a complex expression, must always get the same letter. \\n    - Step 3: tabulate a truth table T3 of all possible truth values of the propositions.\\n    - Step 4: Filter all those rows from table T3 with (i) the conjunction of all sentences \\n      from column \"complex\" is true, and (ii) the conclusion is false. \\n      Do not change values in the rows of T3. The truth value of each proposition with a letter must remain the same even in a complex expression. Make sure that all sentences of complex are true.\\n    - Step 5: control, whether the conjunction of all complex sentences is true. If there is no row left in the last step, the argument ist valid.\\n    - answer: either valid or not valid.\\n    \\n',\n",
       "  'params': ['argument']},\n",
       " {'file': 'argument_valid.j2',\n",
       "  'content': 'You get a text with several sentences. Your task is \\n\\n(a) to segment the text into numbered sentences.\\n(b) to standardize each sentence into a propositional function with propositions (additionally named by unambigous capital letters P,Q,...) connected by propositional functions and,or,iff,not,therefore. Resolve subclauses into normal form.\\n(c) propositions with the same meaning must get the same letter as name. The conclusion gets a letter as well.\\n(d) generate a propositional function \"if conj then concl\" of (i) a conclusion concl from the sentence with an inference operator \"therefore\" (ii) a conjunction conj of all previous sentences except the conclusion and generate a truth table for that function\\n\\nThe text is:\\\\n\\\\n {{argument}}\\\\n',\n",
       "  'params': ['argument']},\n",
       " {'file': 'output_json.j2',\n",
       "  'content': 'All steps of your explanation are given as JSON object. use the format:\\n\\n```json\\n{\\n  \"step1\": markdown1,\\n  \"step2\": markdown2,\\n  \"answer\": either \"valid/not valid\"\\n}\\n```\\n',\n",
       "  'params': []},\n",
       " {'file': 'output_markdown.j2',\n",
       "  'content': 'Output format of {{response}} is quarto markdown format.',\n",
       "  'params': ['response']},\n",
       " {'file': 'output_md2html.j2',\n",
       "  'content': 'Convert this markdown text to html. \\n\\n{{markdown}}',\n",
       "  'params': ['markdown']},\n",
       " {'file': 'quarto_simple.j2',\n",
       "  'content': '---\\ntitle: {{title}}\\nauthor: {{author}}\\nformat:\\n  html:\\n    standalone: true\\n    theme: cerulean\\n    toc: true\\n    html-math-method: katex\\n    toc-title: Contents\\n    toc-location: left\\n    number-sections: true\\n---\\n\\n{{result}}\\n',\n",
       "  'params': ['title', 'author', 'result']}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.tmpls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You get a text with several sentences. Your task is \n",
       "\n",
       "(a) to segment the text into numbered sentences.\n",
       "(b) to standardize each sentence into a propositional function with propositions (additionally named by unambigous capital letters P,Q,...) connected by propositional functions and,or,iff,not,therefore. Resolve subclauses into normal form.\n",
       "(c) propositions with the same meaning must get the same letter as name. The conclusion gets a letter as well.\n",
       "(d) generate a propositional function \"if conj then concl\" of (i) a conclusion concl from the sentence with an inference operator \"therefore\" (ii) a conjunction conj of all previous sentences except the conclusion and generate a truth table for that function\n",
       "\n",
       "The text is:\\n\\n If mankind produces too much CO2, the water level of the ocean rises. \n",
       "The standard of living in Italy is very high and mankind produces too much CO2. \n",
       "The standard of living in India is not as high as in Italy. \n",
       "Tomorrow there are elections in India.\n",
       "The standard of living does not lead to too much CO2.\n",
       "Therefore the water level rises.\n",
       "\\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validity = t.render('argument_valid', argument=text)\n",
    "Markdown(validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen model: llama3:70b, Vendor: Ollama\n"
     ]
    },
    {
     "ename": "ResponseError",
     "evalue": "llama runner process has terminated: signal: killed ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResponseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m r,_\u001b[38;5;241m=\u001b[39m\u001b[43minstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidity\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dropbox/202x/2024projects/ai/instruct.py:70\u001b[0m, in \u001b[0;36minstruct\u001b[0;34m(sys_mess, instruct, model_short)\u001b[0m\n\u001b[1;32m     67\u001b[0m     gg \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m vendor \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mollama\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 70\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msys_mess\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minstruct\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     gg \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m vendor \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlm-studio\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/lmu_ai/lib/python3.12/site-packages/ollama/_client.py:177\u001b[0m, in \u001b[0;36mClient.chat\u001b[0;34m(self, model, messages, stream, format, options, keep_alive)\u001b[0m\n\u001b[1;32m    174\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;241m:=\u001b[39m message\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    175\u001b[0m     message[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [_encode_image(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/chat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m  \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mformat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkeep_alive\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m  \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m  \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lmu_ai/lib/python3.12/site-packages/ollama/_client.py:97\u001b[0m, in \u001b[0;36mClient._request_stream\u001b[0;34m(self, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request_stream\u001b[39m(\n\u001b[1;32m     92\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     93\u001b[0m   \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m     94\u001b[0m   stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     95\u001b[0m   \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     96\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Mapping[\u001b[38;5;28mstr\u001b[39m, Any], Iterator[Mapping[\u001b[38;5;28mstr\u001b[39m, Any]]]:\n\u001b[0;32m---> 97\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/anaconda3/envs/lmu_ai/lib/python3.12/site-packages/ollama/_client.py:73\u001b[0m, in \u001b[0;36mClient._request\u001b[0;34m(self, method, url, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m   response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 73\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[0;31mResponseError\u001b[0m: llama runner process has terminated: signal: killed "
     ]
    }
   ],
   "source": [
    "r,_=instruct(msg,validity,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Let's break down the text into numbered sentences:\n",
       "\n",
       "1. If mankind produces too much CO2, the water level of the ocean rises.\n",
       "2. The standard of living in Italy is very high and mankind produces too much CO2.\n",
       "3. The standard of living in India is not as high as in Italy.\n",
       "4. Tomorrow there are elections in India.\n",
       "5. The standard of living does not lead to too much CO2.\n",
       "6. Therefore the water level rises.\n",
       "\n",
       "Now, let's standardize each sentence into a propositional function:\n",
       "\n",
       "1. P → Q (If mankind produces too much CO2, the water level of the ocean rises.)\n",
       "2. R ∧ P (The standard of living in Italy is very high and mankind produces too much CO2.)\n",
       "3. ¬(S ≥ R) (The standard of living in India is not as high as in Italy.)\n",
       "4. T (Tomorrow there are elections in India.)\n",
       "5. ¬(R → P) (The standard of living does not lead to too much CO2.)\n",
       "6. ∴ Q (Therefore the water level rises.)\n",
       "\n",
       "Now, let's generate a propositional function \"if conj then concl\" and create a truth table:\n",
       "\n",
       "Conjunction of premises: (P → Q) ∧ (R ∧ P) ∧ ¬(S ≥ R) ∧ T ∧ ¬(R → P)\n",
       "Conclusion: Q\n",
       "\n",
       "Propositional function: ((P → Q) ∧ (R ∧ P) ∧ ¬(S ≥ R) ∧ T ∧ ¬(R → P)) → Q\n",
       "\n",
       "Here is the truth table:\n",
       "\n",
       "| P | Q | R | S | T | (P → Q) | (R ∧ P) | ¬(S ≥ R) | ¬(R → P) | ((P → Q) ∧ (R ∧ P) ∧ ¬(S ≥ R) ∧ T ∧ ¬(R → P)) → Q |\n",
       "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
       "| T | T | T | T | T | T | T | T | T | T |\n",
       "| T | T | T | T | F | T | T | T | T | T |\n",
       "| T | T | T | F | T | T | T | T | T | T |\n",
       "| T | T | T | F | F | T | T | T | T | T |\n",
       "| T | F | T | T | T | F | T | T | T | F |\n",
       "| T | F | T | T | F | F | T | T | T | F |\n",
       "| T | F | T | F | T | F | T | T | T | F |\n",
       "| T | F | T | F | F | F | T | T | T | F |\n",
       "| T | F | F | T | T | F | F | T | T | F |\n",
       "| T | F | F | T | F | F | F | T | T | F |\n",
       "| T | F | F | F | T | F | F | T | T | F |\n",
       "| T | F | F | F | F | F | F | T | T | F |\n",
       "| F | T | T | T | T | T | F | T | T | T |\n",
       "| F | T | T | T | F | T | F | T | T | T |\n",
       "| F | T | T | F | T | T | F | T | T | T |\n",
       "| F | T | T | F | F | T | F | T | T | T |\n",
       "| F | T | F | T | T | T | F | T | T | T |\n",
       "| F | T | F | T | F | T | F | T | T | T |\n",
       "| F | T | F | F | T | T | F | T | T | T |\n",
       "| F | T | F | F | F | T | F | T | T | T |\n",
       "| F | F | T | T | T | T | F | T | T | F |\n",
       "| F | F | T | T | F | T | F | T | T | F |\n",
       "| F | F | T | F | T | T | F | T | T | F |\n",
       "| F | F | T | F | F | T | F | T | T | F |\n",
       "| F | F | F | T | T | T | F | T | T | F |\n",
       "| F | F | F | T | F | T | F | T | T | F |\n",
       "| F | F | F | F | T | T | F | T | T | F |\n",
       "| F | F | F | F | F | T | F | T | T | F |\n",
       "\n",
       "The argument is valid if the conclusion (Q) is true in all rows where the premises are true. In this case, the argument is valid."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(r))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
